{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resumes/resume_1.txt',\n",
       " 'resumes/resume_2.txt',\n",
       " 'resumes/resume_11.txt',\n",
       " 'resumes/resume_10.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = glob.glob('resumes/*.txt')\n",
    "fname1 = filenames[0]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REDACTED 1\\nREDACTED\\nWell-rounded full-stack engineer with strong communication and people skills. Software Engineering Intern at \\nCapital One for summer 2019. \\nE\\nDUCATION\\nStanford University (Class of 2020)\\nB.S. Symbolic Systems (Concentration in Human\\n-\\nComputer Interaction) | Minor in Data Science\\nGPA: 3.8\\nStanford, CA\\n   September 2016 \\n-\\n Present\\nR\\nELEVANT \\nE\\nXPERIENCES\\nSoftware Engineering Intern \\n       Stanford, CA\\nCapital One\\nJune 2019 \\n–\\n Present\\n●\\nFull\\n-\\nstack engineer in the Retail an'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname1\n",
    "with open(fname1, 'r') as f:\n",
    "    text = f.read()\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python string methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REDACTED'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split('\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text.split('\\n')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['redacted',\n",
       " '1',\n",
       " 'redacted',\n",
       " 'well-rounded',\n",
       " 'full-stack',\n",
       " 'engineer',\n",
       " 'with',\n",
       " 'strong',\n",
       " 'communication',\n",
       " 'and']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower().split()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/utopia3/dc326/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/utopia3/dc326/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['redacted',\n",
       " '1',\n",
       " 'redacted',\n",
       " 'well-rounded',\n",
       " 'full-stack',\n",
       " 'engineer',\n",
       " 'with',\n",
       " 'strong',\n",
       " 'communication',\n",
       " 'and']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(text.lower())\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['redacted 1\\nredacted\\nwell-rounded full-stack engineer with strong communication and people skills.',\n",
       " 'software engineering intern at \\ncapital one for summer 2019. \\ne\\nducation\\nstanford university (class of 2020)\\nb.s.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = nltk.tokenize.sent_tokenize(text.lower())\n",
    "sents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resumes/resume_1.txt',\n",
       " 'resumes/resume_2.txt',\n",
       " 'resumes/resume_11.txt',\n",
       " 'resumes/resume_10.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = glob.glob('resumes/*.txt')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['redacted', '1', 'redacted', 'well-rounded', 'full-stack']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = list()\n",
    "for fname in filenames:\n",
    "    with open(fname, 'r') as f:\n",
    "        text = f.read()\n",
    "    words = nltk.tokenize.word_tokenize(text.lower())\n",
    "    docs.append(words)\n",
    "docs[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : ['redacted', '1', 'redacted']\n",
      "1 : ['redacted', '2', 'education']\n",
      "1 : ['redacted', '11', 'redacted']\n",
      "2 : ['redacted', '10', 'redacted']\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    cts = Counter(doc)\n",
    "    print(cts['leadership'], ':', doc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['redacted', '1'] : 3\n",
      "['redacted', '2'] : 1\n",
      "['redacted', '11'] : 1\n",
      "['redacted', '10'] : 2\n"
     ]
    }
   ],
   "source": [
    "keywords = ['leader', 'leadership', 'president', 'chair']\n",
    "for doc in docs:\n",
    "    cts = Counter(doc)\n",
    "    num = sum([cts[kw] for kw in keywords])\n",
    "    print(doc[:2], ':', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['redacted', '1'] : ['●', '”', '“', '’', '‘']\n",
      "['redacted', '2'] : ['•', '’', '|', 'workplace', 'working']\n",
      "['redacted', '11'] : ['•', '”', '“', '’', 'young']\n",
      "['redacted', '10'] : ['●', '’', 'york', 'years', 'xcode']\n"
     ]
    }
   ],
   "source": [
    "# most common words\n",
    "for doc in docs:\n",
    "    cts = Counter(doc)\n",
    "    topwords = list(sorted(cts, reverse=True))\n",
    "    print(doc[:2], ':', topwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('redacted', 'VBN'), ('1', 'CD'), ('redacted', 'JJ'), ('well-rounded', 'JJ'), ('full-stack', 'NN')]\n",
      "[('redacted', 'VBN'), ('2', 'CD'), ('education', 'NN'), ('univ', 'JJ'), ('ersity', 'NN')]\n",
      "[('redacted', 'VBN'), ('11', 'CD'), ('redacted', 'VBN'), ('computer', 'NN'), ('skills', 'NNS')]\n",
      "[('redacted', 'VBN'), ('10', 'CD'), ('redacted', 'JJ'), ('education', 'NN'), ('may', 'MD')]\n"
     ]
    }
   ],
   "source": [
    "# part-of-speech tag filtering\n",
    "KEEP = ['NN', 'VBN']\n",
    "for doc in docs:\n",
    "    pos = nltk.pos_tag(doc)\n",
    "    use_tok = [tok for tok,tag in pos if tag in KEEP]\n",
    "    print(pos[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introductory', 'computer', 'science', 'course', 'in', 'python', '(', 'cs106a', ')', 'march']\n",
      "['projects', ':', '●', 'chatbot', '(', 'python', ')', '-', 'implemented', 'nlp']\n",
      "[':', 'java', ',', 'javascript', ',', 'python', ',', 'html', ',', 'css']\n",
      "['analytics', 'gpa', ':', '3.94', 'skills', 'python', ',', 'java', ',', 'sql']\n",
      "['novel', 'anomaly', 'detection', 'model', 'in', 'python', 'that', 'flags', 'unhealthy', 'sensors']\n",
      "['for', 'energy', 'access', 'knowledge', 'using', 'python', '•', 'created', 'algorithm', 'that']\n",
      "['proficient', 'in', ':', 'java', ',', 'python', ',', 'c', '#', ',']\n",
      "['●', 'taught', 'myself', 'react', 'and', 'python', 'to', 'build', 'a', 'personal']\n",
      "['skills', 'languages', ':', 'c/c++', ',', 'python', ',', 'matlab', ',', 'r']\n"
     ]
    }
   ],
   "source": [
    "# words surrounding a given target word\n",
    "for doc in docs:\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i] == 'python':\n",
    "            print(doc[i-5:i+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redacted 1 : 3.8\n",
      "redacted 2 : 3.88\n",
      "redacted 2 : 3.92\n",
      "redacted 2 : 3.94\n"
     ]
    }
   ],
   "source": [
    "# search for GPA of these documents\n",
    "for doc in docs:\n",
    "    for tok in doc:\n",
    "        isint, isfloat = False, False\n",
    "        try:\n",
    "            num = float(tok)\n",
    "            isfloat = True\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            num = int(tok)\n",
    "            isint = True\n",
    "        except:\n",
    "            pass\n",
    "        if isfloat and not isint and float(tok) > 2.0 and float(tok) < 5.0:\n",
    "            print(' '.join(doc[:2]), ':', float(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
