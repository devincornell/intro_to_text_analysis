{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texts as Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 400, 150)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_paragraphs(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        text = f.read()\n",
    "    paragraphs = [p for p in text.split('\\n\\n') if len(p) > 0]\n",
    "    return paragraphs\n",
    "\n",
    "trump_par_texts = read_paragraphs('nss/trump_nss.txt')\n",
    "obama_par_texts = read_paragraphs('nss/obama_nss.txt')\n",
    "par_texts = trump_par_texts + obama_par_texts\n",
    "k = len(trump_par_texts)\n",
    "len(par_texts), len(trump_par_texts), len(obama_par_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Formula\n",
    "A common step to many text analysis algorithms is to first convert the raw text into sets of tokens. Spacy does most of the work here, there are just a few decisions that need to be made depending on the application: which tokens to include and how to represent the tokens as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tok(tok):\n",
    "    '''Convert spacy token object to string.'''\n",
    "    number_ents = ('NUMBER','MONEY','PERCENT','QUANTITY','CARDINAL','ORDINAL')\n",
    "    if tok.ent_type_ == '':\n",
    "        return tok.text.lower()\n",
    "    elif tok.ent_type_ in number_ents:\n",
    "        return tok.ent_type_\n",
    "    else:\n",
    "        return tok.text\n",
    "    \n",
    "def use_tok(tok):\n",
    "    '''Decide to use token or not.'''\n",
    "    return tok.is_ascii and not tok.is_space and len(tok.text.strip()) > 0\n",
    "    \n",
    "def parse_doc(doc):\n",
    "    # combine multi-word entities into their own tokens (just a formula)\n",
    "    for ent in doc.ents:\n",
    "        ent.merge(tag=ent.root.tag_, ent_type=ent.root.ent_type_)\n",
    "    return [parse_tok(tok) for tok in doc if use_tok(tok)]\n",
    "\n",
    "tokenized_pars = [parse_doc(par) for par in nlp.pipe(par_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'America', 'that', 'is', 'safe']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first paragraph, first five tokens\n",
    "tokenized_pars[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words and Document-Term Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2287, scipy.sparse.csr.csr_matrix, (550, 2287), (550, 2287))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_tf = 2\n",
    "vectorizer = CountVectorizer(tokenizer = lambda x: x, preprocessor=lambda x:x, min_df=min_tf)\n",
    "corpus = vectorizer.fit_transform(tokenized_pars)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "DOCBOW = corpus.toarray()\n",
    "len(vocab), type(corpus), corpus.shape, DOCBOW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', ',', '-', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 6, 0, 3],\n",
       "       [0, 0, 3, 0, 1],\n",
       "       [0, 0, 6, 1, 4],\n",
       "       [0, 0, 8, 1, 4],\n",
       "       [0, 0, 4, 0, 2]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab[:5])\n",
    "DOCBOW[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove documents that have none of the selected vocab words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((443, 2287), 325, 443, 443, 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_wordcount = 10\n",
    "zerosel = DOCBOW.sum(axis=1) < min_wordcount\n",
    "zeroind = np.argwhere(zerosel)[:,0]\n",
    "tokenized_pars = [toks for i,toks in enumerate(tokenized_pars) if i not in zeroind]\n",
    "par_texts = [par for i,par in enumerate(par_texts) if i not in zeroind]\n",
    "k = k - (zeroind < k).sum()\n",
    "DOCBOW = DOCBOW[~zerosel]\n",
    "\n",
    "DOCBOW.shape, k, len(tokenized_pars), len(par_texts), (DOCBOW.sum(axis=1)==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((443, 1517), 770, 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_cols = (DOCBOW[:k].sum(axis=0) > 0) & (DOCBOW[k:].sum(axis=0) > 0)\n",
    "rm_wordids = np.argwhere(~valid_cols)[:,0]\n",
    "vocab = [w for i,w in enumerate(vocab) if i not in rm_wordids]\n",
    "DOCBOW = DOCBOW[:,valid_cols]\n",
    "DOCBOW.shape, (~valid_cols).sum(), (DOCBOW.sum(axis=0)==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reduce vocabulary to words which appear at least once in both corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', ',', '-', '.', '70 years', ':', ';', 'ASEAN', 'Afghan']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  6,  0,  3,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  3,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  6,  1,  4,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  8,  1,  4,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  4,  0,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  5,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  3,  0,  3,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  1,  0,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 13,  0,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  9,  1,  3,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "DOCBOW[:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using bag of words, we now compare word distributions averaged across the documents from Trump and Obama. Here we present the words that are more likely to be in the Trump NSS compared to Obama's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": (3.05), compete (2.30), missile (2.30), under (2.23), conditions (2.15), nation (2.10), industry (2.06), seeks (2.06), liberty (2.06), immigration (2.06), Americans (2.03), continues (1.96), sovereign (1.96), minded (1.96), adversaries (1.93), life (1.91), encourage (1.86), communications (1.86), was (1.86), intellectual (1.86), deterrence (1.86), want (1.86), identify (1.86), base (1.74), understand (1.74), criminals (1.74), principles (1.74), technologies (1.70), instability (1.61), projects (1.61)\n"
     ]
    }
   ],
   "source": [
    "topn = 30\n",
    "trump_cts, obama_cts = DOCBOW[:k].sum(axis=0), DOCBOW[k:].sum(axis=0)\n",
    "logdiff = np.log( (trump_cts/trump_cts.sum()) / (obama_cts / obama_cts.sum()))\n",
    "indices = logdiff.argsort()[-topn:][::-1]\n",
    "print(', '.join(['{} ({:.2f})'.format(vocab[idx],logdiff[idx]) for idx in indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "Because topic models are computed directly from document-term matrices, I demonstrate the use of both the NMF and LDA algorithms. After computing each model, I then compute the log ratio of probabilities of subcorpora being associated with each topic. Larger values mean Trump's documents are more closely associated with the topic while more negative values are more closely associated with Obama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 1517), (443, 10))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-negative matrix factorization (similar to pca but for only positive-entry matrices)\n",
    "nmf_model = NMF(n_components=10).fit(DOCBOW)\n",
    "doc_topics = nmf_model.transform(DOCBOW)\n",
    "topic_words = nmf_model.components_\n",
    "topic_words.shape, doc_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40784656, -0.61119085, -0.83861523, -0.73672017, -0.40889217,\n",
       "       -0.46963959, -0.84349726, -0.17227802, -1.79445676, -0.40865547])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for nmf compare distributions between sources\n",
    "trump_av = doc_topics[:k].mean(axis=0)\n",
    "obama_av = doc_topics[k:].mean(axis=0)\n",
    "logratio = np.log(trump_av/obama_av)\n",
    "logratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 1517), (443, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-negative matrix factorization (similar to pca but for only positive-entry matrices)\n",
    "lda_model = LatentDirichletAllocation(n_components=10).fit(DOCBOW)\n",
    "doc_topics = lda_model.transform(DOCBOW)\n",
    "topic_words = lda_model.components_\n",
    "topic_words.shape, doc_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61771866,  0.61771866,  0.06282455,  1.10934547,  0.09945917,\n",
       "        2.11334549,  1.29630909,  1.20561872, -0.10447677, -0.52768139])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for nmf compare distributions between sources\n",
    "trump_av = doc_topics[:k].mean(axis=0)\n",
    "obama_av = doc_topics[k:].mean(axis=0)\n",
    "logratio = np.log(trump_av/obama_av)\n",
    "logratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Mutual Information\n",
    "The [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) calculates the level of association between two variables, document and word in this case (as designated by the 2-dimensional distribution), by controlling for both the frequency of a word and the number of words in a document. Higher values mean that the word is more uniquely associated with the document statistically than not.\n",
    "\n",
    "Positive pointwise mutual information is a variant of PMI which sets negative values (words which are less associated with documents than expected) to zero. While we loose some information here, this solves the problem of -infinity values caused by taking log(0) and has shown to still be a robust measure.\n",
    "Levy, Goldberg, Dagan (2015) _Improving Distributional Similarity with Lessons Learned from Word Embeddings_ ([link])(https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textfields # from included script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(443, 1517)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 6.77146012, 0.        , 6.4146238 ],\n",
       "       [0.        , 0.        , 6.71487892, 0.        , 6.05394382],\n",
       "       [0.        , 0.        , 6.43861906, 7.03590552, 6.29754426],\n",
       "       [0.        , 0.        , 6.70544636, 7.08697126, 6.34861   ],\n",
       "       [0.        , 0.        , 6.63873119, 0.        , 6.28189487]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPMI = textfields.calc_ppmi(DOCBOW)\n",
    "print(PPMI.shape)\n",
    "PPMI[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of PMI is that you can go back to the original documents to examine the words most closely associated with them compared to all other docs in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "putting (12.06) uphold (10.39) foundation (10.29) liberty (10.06) enduring (9.99) \n",
      " An America that is safe, prosperous, and free at home is an America with the strength, confidence, and will to lead abroad. It is an America that can preserve peace, uphold liberty , and create enduring advantages for the American people. Putting America first is the duty of our government and the foundation for U.S. leadership in the world.\n"
     ]
    }
   ],
   "source": [
    "target_docid = 0\n",
    "temp_PPMI = PPMI.copy()\n",
    "for i in range(5):\n",
    "    idx = temp_PPMI[target_docid,:].argmax()\n",
    "    print('{} ({:.2f})'.format(vocab[idx], temp_PPMI[target_docid,idx]), end=' ')\n",
    "    temp_PPMI[target_docid,idx] = 0\n",
    "print('\\n', par_texts[target_docid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Co-Occurrence Matrices, Word Embeddings\n",
    "Now we can consider document co-occurrences. Co-occurrences in general can be broken down at the n-gram, sentence, paragraph, or document level, in this example we will demonstrate using only document frequencies. This is because the co-occurrence matrix can be constructed using only the docbow matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443, 1517)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOCBOW.shape # dimensionality of the original co-occurrence matrix (#docs x #vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1517, 1517)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COOC = DOCBOW.T.dot(DOCBOW)\n",
    "COOC.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row and each column corresponds to a token, and the entries indiciate the number of times two tokens appeared in the same document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', ',', '-', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   42,    43,   211, ...,     6,     0,     0],\n",
       "       [   43,    47,   237, ...,     6,     0,     0],\n",
       "       [  211,   237, 15111, ...,   667,    51,    13],\n",
       "       ...,\n",
       "       [    6,     6,   667, ...,   153,     1,     0],\n",
       "       [    0,     0,    51, ...,     1,    10,     1],\n",
       "       [    0,     0,    13, ...,     0,     1,     2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab[:5])\n",
    "COOC[0:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now by using PPMI and SVD we can collapse the co-occurrence matrix into a smaller number of dimensions and create word vectors based on the co-occurrence counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1517, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.81846888, -0.05946985, -0.17225793,  0.10743349, -0.19500819])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dim = 100\n",
    "PPMI = textfields.calc_ppmi(COOC)\n",
    "SVD = textfields.calc_svd(PPMI, n_dim)\n",
    "SVD = SVD / np.linalg.norm(SVD, axis=1)[:,np.newaxis] # normalize vectors to unit length\n",
    "print(SVD.shape)\n",
    "SVD[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68797   , 0.68542698, 0.64455407, 0.65570018, 0.64272072])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = vocab.index('freedom') # index of matrix row corresponding to freedom\n",
    "dists = SVD.dot(SVD[ind])\n",
    "dists[:5] # cosine similarity between each word and 'freedom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['individual',\n",
       " 'between',\n",
       " 'law',\n",
       " 'open',\n",
       " 'who',\n",
       " 'illegal',\n",
       " 'actors',\n",
       " 'society',\n",
       " 'those',\n",
       " 'civil']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = dists.argsort()[::-1][1:] # indices of closest words, removing the word itself (always 1)\n",
    "topwords = [vocab[idx] for idx in order]\n",
    "topwords[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
