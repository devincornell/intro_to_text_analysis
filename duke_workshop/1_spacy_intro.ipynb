{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Basics\n",
    "Devin J. Cornell, Nov 2019\n",
    "\n",
    "To begin:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text File, Convert to Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An America that is safe, prosperous, and free at home is an America with the strength, confidence, and will to lead abroad. It is an America that can preserve peace, uphold liberty , and create enduring advantages for the American people. Putting America first is the duty of our government and the foundation for U.S. leadership in the world.\n",
      "\n",
      "A strong America is in the vital interests of not only the American people, but also those around the world who want to partner with the United States in p ...\n"
     ]
    }
   ],
   "source": [
    "with open('nss/trump_nss.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:500], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, str, 400)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = [p for p in text.split('\\n\\n') if len(p) > 0]\n",
    "type(paragraphs), type(paragraphs[0]), len(paragraphs)\n",
    "# each paragraph is a string in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spacy\n",
    "For more language models see [Spacy Documentation on Models & Languages](https://spacy.io/usage/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f9414349dd8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy # import the package\n",
    "nlp = spacy.load('en') # load a language model\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work With Spacy Doc Object\n",
    "A spacy doc object is returned after parsing a document using `nlp()`. You can see the object properties in the [spacy Doc object documentation](https://spacy.io/api/doc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here the first paragraph is parsed with spacy and the result is in the variable \"doc\"\n",
    "doc = nlp(paragraphs[0])\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can treat doc as an iterator across [Token objects](https://spacy.io/api/token). See a list of token attributes in the [spacy token attributes documentation](https://spacy.io/api/token#attributes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n",
      "An\n",
      "America\n",
      "that\n"
     ]
    }
   ],
   "source": [
    "print(type(doc[0]))\n",
    "for tok in doc[:3]:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Token Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An: lower-case string: \"an\", whitespace: \" \", string with whitespace: \"An \", lemma: \"an\", prefix: \"A\", suffix: \"An\"\n",
      "\n",
      "America: lower-case string: \"america\", whitespace: \" \", string with whitespace: \"America \", lemma: \"America\", prefix: \"A\", suffix: \"ica\"\n",
      "\n",
      "that: lower-case string: \"that\", whitespace: \" \", string with whitespace: \"that \", lemma: \"that\", prefix: \"t\", suffix: \"hat\"\n",
      "\n",
      "is: lower-case string: \"is\", whitespace: \" \", string with whitespace: \"is \", lemma: \"be\", prefix: \"i\", suffix: \"is\"\n",
      "\n",
      "safe: lower-case string: \"safe\", whitespace: \"\", string with whitespace: \"safe\", lemma: \"safe\", prefix: \"s\", suffix: \"afe\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tok in doc[:5]:\n",
    "    print('{}: '\n",
    "          'lower-case string: \"{}\", '\n",
    "          'whitespace: \"{}\", '\n",
    "          'string with whitespace: \"{}\", '\n",
    "          'lemma: \"{}\", '\n",
    "          'prefix: \"{}\", '\n",
    "          'suffix: \"{}\"'\n",
    "          '\\n'.format(tok.text, tok.lower_, tok.whitespace_, tok.text_with_ws, tok.lemma_, tok.prefix_, tok.suffix_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An America that is safe, prosperous, and free at home is an America with the strength, confidence, and will to lead abroad. It is an America that can preserve peace, uphold liberty , and create enduring advantages for the American people. Putting America first is the duty of our government and the foundation for U.S. leadership in the world.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can put doc back into string by using text_with_ws\n",
    "''.join([tok.text_with_ws for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_stop: An\n",
      "is_stop: that\n",
      "is_stop: is\n",
      "is_punct: ,\n",
      "is_punct: ,\n",
      "is_stop: and\n",
      "is_stop: at\n",
      "is_stop: is\n",
      "is_stop: an\n",
      "is_stop: with\n",
      "is_stop: the\n",
      "is_punct: ,\n",
      "is_punct: ,\n",
      "is_stop: and\n",
      "is_stop: will\n",
      "is_stop: to\n",
      "is_punct: .\n",
      "is_stop: It\n",
      "is_stop: is\n",
      "is_stop: an\n",
      "is_stop: that\n",
      "is_stop: can\n",
      "is_punct: ,\n",
      "is_punct: ,\n",
      "is_stop: and\n",
      "is_stop: for\n",
      "is_stop: the\n",
      "is_punct: .\n",
      "is_stop: first\n",
      "is_stop: is\n",
      "is_stop: the\n",
      "is_stop: of\n",
      "is_stop: our\n",
      "is_stop: and\n",
      "is_stop: the\n",
      "is_stop: for\n",
      "is_upper: U.S.\n",
      "is_stop: in\n",
      "is_stop: the\n",
      "is_punct: .\n"
     ]
    }
   ],
   "source": [
    "# this shows a few token flags assigned to tokens.\n",
    "for tok in doc:\n",
    "    if tok.is_punct:\n",
    "        print('is_punct:', tok)\n",
    "    elif tok.is_digit:\n",
    "        print('is_digit:', tok)\n",
    "    elif tok.is_upper:\n",
    "        print('is_upper:', tok)\n",
    "    elif tok.is_stop:\n",
    "        print('is_stop:', tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "You can extract named entity information by using the `tok.ent_type_` attribute. You can see a full list of named entity types in the [spacy named entity documentation](https://spacy.io/api/annotation#named-entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An, type: \n",
      "America, type: GPE\n",
      "that, type: \n",
      "is, type: \n",
      "safe, type: \n"
     ]
    }
   ],
   "source": [
    "# Notice how tok._ent_type_ == '' for cases where the token is not an entity\n",
    "for tok in doc[:5]:\n",
    "    print('{}, type: {}'.format(tok, tok.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "America, type: GPE\n",
      "America, type: GPE\n",
      "America, type: GPE\n",
      "American, type: NORP\n",
      "America, type: GPE\n",
      "first, type: ORDINAL\n",
      "U.S., type: GPE\n"
     ]
    }
   ],
   "source": [
    "# now only print entities\n",
    "for tok in doc:\n",
    "    if tok.ent_type_ != '':\n",
    "        print('{}, type: {}'.format(tok, tok.ent_type_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences and Parse Trees\n",
    "Spacy also has the ability to parse sentences for gramattical structures. This includes breaking tokens into groups of tokens consituting sentences, assigning Part-of-Speech (POS) tags to each word, and building a Dependency Parse Tree for detailed gramattical structure. For information on parse trees, see the [dependency type list](https://spacy.io/api/annotation#dependency-parsing) and see the [displacy visualization](https://explosion.ai/demos/displacy) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An America that is safe, prosperous, and free at home is an America with the strength, confidence, and will to lead abroad.\n",
      "It is an America that can preserve peace, uphold liberty , and create enduring advantages for the American people.\n",
      "Putting America first is the duty of our government and the foundation for U.S. leadership in the world.\n"
     ]
    }
   ],
   "source": [
    "# just print each sentence\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An (DET) America (PROPN) that (PRON) is (AUX) safe (ADJ) , (PUNCT) prosperous (ADJ) , (PUNCT) and (CCONJ) free (ADJ) at (ADP) home (NOUN) is (AUX) an (DET) America (PROPN) with (ADP) the (DET) strength (NOUN) , (PUNCT) confidence (NOUN) , (PUNCT) and (CCONJ) will (AUX) to (PART) lead (VERB) abroad (ADV) . (PUNCT)\n"
     ]
    }
   ],
   "source": [
    "# extract part of speech from each spacy token\n",
    "for sent in doc.sents:\n",
    "    print(' '.join(['{} ({})'.format(tok,tok.pos_) for tok in sent]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An (DT) America (NNP) that (WDT) is (VBZ) safe (JJ) , (,) prosperous (JJ) , (,) and (CC) free (JJ) at (IN) home (NN) is (VBZ) an (DT) America (NNP) with (IN) the (DT) strength (NN) , (,) confidence (NN) , (,) and (CC) will (MD) to (TO) lead (VB) abroad (RB) . (.)\n"
     ]
    }
   ],
   "source": [
    "# there is also a more fine-grained version\n",
    "for sent in doc.sents:\n",
    "    print(' '.join(['{} ({})'.format(tok,tok.tag_) for tok in sent]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Tree Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at home ',\n",
       " 'with the strength, confidence',\n",
       " 'for the American people',\n",
       " 'of our government and the foundation for U.S. leadership in the world',\n",
       " 'for U.S. leadership in the world',\n",
       " 'in the world']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all prepositional phrases\n",
    "def prep_phrases(doc):\n",
    "    phrases = list()\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == 'ADP':\n",
    "            pp = ''.join([t.orth_ + t.whitespace_ for t in tok.subtree])\n",
    "            phrases.append(pp)\n",
    "    return phrases\n",
    "prep_phrases(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(America, is, None), (It, is, None), (None, is, None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def noun_verb_pairs(doc):\n",
    "    nounverbs = list()\n",
    "    for tok in doc:\n",
    "        if tok.dep_ == 'ROOT':\n",
    "            nounverbs.append((child(tok,'nsubj'),tok,child(tok,'dobj')))\n",
    "    return nounverbs\n",
    "\n",
    "def child(tok, dep): # helper function\n",
    "    for c in tok.children:\n",
    "        if c.dep_== dep:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "noun_verb_pairs(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Multiple Documents\n",
    "Here I show several of the previous examples that have been applied to the entire set of documents instead of just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, list, spacy.tokens.doc.Doc)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform spacy parsing\n",
    "parsed_pars = list(nlp.pipe(paragraphs))\n",
    "len(parsed_pars), type(parsed_pars), type(parsed_pars[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 1373), (',', 1227), ('.', 995), ('the', 923), ('to', 793), ('of', 582), ('our', 381), ('will', 334), ('that', 269), ('in', 268)]\n"
     ]
    }
   ],
   "source": [
    "# get most frequent words\n",
    "tokens = [tok.text for doc in parsed_pars for tok in doc]\n",
    "tok_cts = Counter(tokens)\n",
    "sort_cts = list(sorted(tok_cts.items(), key=lambda x: x[1], reverse=True))\n",
    "print(sort_cts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('United', 223), ('States', 219), ('the', 182), ('U.S.', 124), ('American', 121), ('The', 116), ('America', 85), ('China', 33), ('Americans', 32), ('Russia', 23)]\n"
     ]
    }
   ],
   "source": [
    "# get all entities from all documents, count their frequency of use\n",
    "all_entities = [tok.text for doc in parsed_pars for tok in doc if tok.ent_type_!='']\n",
    "ent_cts = Counter(all_entities)\n",
    "sort_cts = list(sorted(ent_cts.items(), key=lambda x: x[1], reverse=True))\n",
    "print(sort_cts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('We', 'work'), 18), (('We', 'encourage'), 9), (('We', 'support'), 9), (('We', 'are'), 8), (('States', 'continue'), 8)]\n"
     ]
    }
   ],
   "source": [
    "# most common noun-verb pairs\n",
    "all_nv = [(n.text,v.text) for doc in parsed_pars for n,v,s in noun_verb_pairs(doc) if n is not None]\n",
    "nv_cts = Counter(all_nv)\n",
    "sort_cts = list(sorted(nv_cts.items(), key=lambda x: x[1], reverse=True))\n",
    "print(sort_cts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
