{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texts as Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 400, 150)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_paragraphs(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        text = f.read()\n",
    "    paragraphs = [p for p in text.split('\\n\\n') if len(p) > 0]\n",
    "    return paragraphs\n",
    "\n",
    "trump_par_texts = read_paragraphs('nss/trump_nss.txt')\n",
    "obama_par_texts = read_paragraphs('nss/obama_nss.txt')\n",
    "par_texts = trump_par_texts + obama_par_texts\n",
    "k = len(trump_par_texts)\n",
    "len(par_texts), len(trump_par_texts), len(obama_par_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Formula\n",
    "A common step to many text analysis algorithms is to first convert the raw text into sets of tokens. Spacy does most of the work here, there are just a few decisions that need to be made depending on the application: which tokens to include and how to represent the tokens as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tok(tok):\n",
    "    '''Convert spacy token object to string.'''\n",
    "    number_ents = ('NUMBER','MONEY','PERCENT','QUANTITY','CARDINAL','ORDINAL')\n",
    "    if tok.ent_type_ == '':\n",
    "        return tok.text.lower()\n",
    "    elif tok.ent_type_ in number_ents:\n",
    "        return tok.ent_type_\n",
    "    else:\n",
    "        return tok.text\n",
    "    \n",
    "def use_tok(tok):\n",
    "    '''Decide to use token or not.'''\n",
    "    return tok.is_ascii and not tok.is_space and len(tok.text.strip()) > 0\n",
    "    \n",
    "def parse_doc(doc):\n",
    "    # combine multi-word entities into their own tokens (just a formula)\n",
    "    for ent in doc.ents:\n",
    "        ent.merge(tag=ent.root.tag_, ent_type=ent.root.ent_type_)\n",
    "    return [parse_tok(tok) for tok in doc if use_tok(tok)]\n",
    "\n",
    "tokenized_pars = [parse_doc(par) for par in nlp.pipe(par_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'America', 'that', 'is', 'safe']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first paragraph, first five tokens\n",
    "tokenized_pars[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words and Document-Term Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524, scipy.sparse.csr.csr_matrix, (550, 524), (550, 524))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_tf = 10\n",
    "vectorizer = CountVectorizer(tokenizer = lambda x: x, preprocessor=lambda x:x, min_df=min_tf)\n",
    "corpus = vectorizer.fit_transform(tokenized_pars)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "COOC = corpus.toarray()\n",
    "len(vocab), type(corpus), corpus.shape, COOC.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reduce vocabulary to words which appear at least once in both corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 507)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_cols = (COOC[:k].sum(axis=0) > 0) & (COOC[k:].sum(axis=0) > 0)\n",
    "rm_wordids = np.argwhere(~valid_cols)[:,0]\n",
    "vocab = [w for i,w in enumerate(vocab) if i not in rm_wordids]\n",
    "COOC = COOC[:,valid_cols]\n",
    "COOC.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove documents that have none of the selected vocab words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((441, 507), 324, 441, 441)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_wordcount = 10\n",
    "zerosel = COOC.sum(axis=1) < min_wordcount\n",
    "zeroind = np.argwhere(zerosel)[:,0]\n",
    "tokenized_pars = [toks for i,toks in enumerate(tokenized_pars) if i not in zeroind]\n",
    "par_texts = [par for i,par in enumerate(par_texts) if i not in zeroind]\n",
    "k = k - (zeroind < k).sum()\n",
    "COOC = COOC[~zerosel]\n",
    "\n",
    "COOC.shape, k, len(tokenized_pars), len(par_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ')', ',', '-', '.', ':', ';', 'Africa', 'America', 'American']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  6,  0,  3,  0,  0,  0,  4,  1],\n",
       "       [ 0,  0,  3,  0,  1,  0,  0,  0,  1,  1],\n",
       "       [ 0,  0,  6,  1,  4,  0,  0,  0,  0,  2],\n",
       "       [ 0,  0,  8,  1,  4,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  4,  0,  2,  0,  0,  0,  0,  1],\n",
       "       [ 0,  0,  5,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  3,  0,  3,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  1,  0,  2,  0,  0,  0,  0,  1],\n",
       "       [ 0,  0, 13,  0,  5,  0,  0,  0,  2,  0],\n",
       "       [ 1,  1,  9,  1,  3,  0,  0,  0,  0,  1]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "COOC[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(COOC.sum(axis=1)==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using bag of words, we now compare word distributions averaged across the documents from Trump and Obama. Here we present the words that are more likely to be in the Trump NSS compared to Obama's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": (3.03), compete (2.27), under (2.20), conditions (2.12), nation (2.08), seeks (2.03), liberty (2.03), industry (2.03), Americans (2.00), minded (1.94), continues (1.94), sovereign (1.94), adversaries (1.90), life (1.89), want (1.83), encourage (1.83), principles (1.71), technologies (1.67), practices (1.58), undermine (1.58), The United States (1.56), improve (1.56), vital (1.51), Today (1.43), companies (1.43), domestic (1.43), operate (1.43), operations (1.43), property (1.34), into (1.34)\n"
     ]
    }
   ],
   "source": [
    "topn = 30\n",
    "trump_cts, obama_cts = COOC[:k].sum(axis=0), COOC[k:].sum(axis=0)\n",
    "logdiff = np.log( (trump_cts/trump_cts.sum()) / (obama_cts / obama_cts.sum()))\n",
    "indices = logdiff.argsort()[-topn:][::-1]\n",
    "print(', '.join(['{} ({:.2f})'.format(vocab[idx],logdiff[idx]) for idx in indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "Because topic models are computed directly from document-term matrices, I demonstrate the use of both the NMF and LDA algorithms. After computing each model, I then compute the log ratio of probabilities of subcorpora being associated with each topic. Larger values mean Trump's documents are more closely associated with the topic while more negative values are more closely associated with Obama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 507), (441, 10))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-negative matrix factorization (similar to pca but for only positive-entry matrices)\n",
    "nmf_model = NMF(n_components=10).fit(COOC)\n",
    "doc_topics = nmf_model.transform(COOC)\n",
    "topic_words = nmf_model.components_\n",
    "topic_words.shape, doc_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.41283237, -0.63728651, -0.84966108, -0.74124851, -0.48178337,\n",
       "       -0.45959257, -0.87741655, -0.15518991, -1.72093414, -0.42493658])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for nmf compare distributions between sources\n",
    "trump_av = doc_topics[:k].mean(axis=0)\n",
    "obama_av = doc_topics[k:].mean(axis=0)\n",
    "logratio = np.log(trump_av/obama_av)\n",
    "logratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 507), (441, 10))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-negative matrix factorization (similar to pca but for only positive-entry matrices)\n",
    "lda_model = LatentDirichletAllocation(n_components=10).fit(COOC)\n",
    "doc_topics = lda_model.transform(COOC)\n",
    "topic_words = lda_model.components_\n",
    "topic_words.shape, doc_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.54148573,  0.64658564,  1.72492766,  1.28210013,  0.09256617,\n",
       "       -0.68714229,  1.15892343, -0.50059194,  1.25416577,  1.66449761])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for nmf compare distributions between sources\n",
    "trump_av = doc_topics[:k].mean(axis=0)\n",
    "obama_av = doc_topics[k:].mean(axis=0)\n",
    "logratio = np.log(trump_av/obama_av)\n",
    "logratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Mutual Information\n",
    "The [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) calculates the level of association between two variables, document and word in this case (as designated by the 2-dimensional distribution), by controlling for both the frequency of a word and the number of words in a document. Higher values mean that the word is more uniquely associated with the document statistically than not.\n",
    "\n",
    "Positive pointwise mutual information is a variant of PMI which sets negative values (words which are less associated with documents than expected) to zero. While we loose some information here, this solves the problem of -infinity values caused by taking log(0) and has shown to still be a robust measure.\n",
    "Levy, Goldberg, Dagan (2015) _Improving Distributional Similarity with Lessons Learned from Word Embeddings_ ([link])(https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matrices # from included script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441, 507)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 5.59187069, 0.        , 5.22512691],\n",
       "       [0.        , 0.        , 5.57527119, 0.        , 4.90442862],\n",
       "       [0.        , 0.        , 5.39127814, 5.97436491, 5.24029597],\n",
       "       [0.        , 0.        , 5.66166589, 6.0289911 , 5.29492211],\n",
       "       [0.        , 0.        , 5.55210853, 0.        , 5.1853647 ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPMI = matrices.calc_ppmi(COOC)\n",
    "print(PPMI.shape)\n",
    "PPMI[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of PMI is that you can go back to the original documents to examine the words most closely associated with them compared to all other docs in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uphold (9.22) foundation (9.12) liberty (8.88) enduring (8.81) confidence (8.79) \n",
      " An America that is safe, prosperous, and free at home is an America with the strength, confidence, and will to lead abroad. It is an America that can preserve peace, uphold liberty , and create enduring advantages for the American people. Putting America first is the duty of our government and the foundation for U.S. leadership in the world.\n"
     ]
    }
   ],
   "source": [
    "target_docid = 0\n",
    "temp_PPMI = PPMI.copy()\n",
    "for i in range(5):\n",
    "    idx = temp_PPMI[target_docid,:].argmax()\n",
    "    print('{} ({:.2f})'.format(vocab[idx], temp_PPMI[target_docid,idx]), end=' ')\n",
    "    temp_PPMI[target_docid,idx] = 0\n",
    "print('\\n', par_texts[target_docid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition of PPMI Matrix\n",
    "This is simply decomposing the PPMI matrix into singular values, which we used to compress the matrix. Note that in cases where your vocab is larger than the number of documents, read the [implementation notes](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html#numpy.linalg.svd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02406945, 0.03615592, 0.02495261, 0.02607884, 0.02111817,\n",
       "       0.02161532, 0.0193136 , 0.0166996 , 0.01554702, 0.01459172,\n",
       "       0.01496909, 0.01428053, 0.01504344, 0.01366774, 0.01302898,\n",
       "       0.01325838, 0.01327273, 0.01362083, 0.01144731, 0.01272128,\n",
       "       0.01213789, 0.01192576, 0.01205809, 0.01139212, 0.0105516 ,\n",
       "       0.01079131, 0.01092207, 0.0110332 , 0.0108154 , 0.010916  ,\n",
       "       0.01073725, 0.00979404, 0.01038939, 0.01052787, 0.0102035 ,\n",
       "       0.00924232, 0.01018133, 0.0102287 , 0.00971629, 0.00972708,\n",
       "       0.00938486, 0.00887874, 0.00909769, 0.00933416, 0.00913616,\n",
       "       0.00886873, 0.00891708, 0.00894251, 0.00803768, 0.00816864,\n",
       "       0.00833853, 0.00833357, 0.00808068, 0.0081198 , 0.00811789,\n",
       "       0.00829316, 0.00790994, 0.00762248, 0.00740023, 0.00774648,\n",
       "       0.00736724, 0.00800482, 0.00715861, 0.00721603, 0.007624  ,\n",
       "       0.00712471, 0.00747347, 0.00732539, 0.00737229, 0.00773569,\n",
       "       0.0069016 , 0.00722822, 0.00685813, 0.00736594, 0.00663901,\n",
       "       0.00662883, 0.00671263, 0.0065913 , 0.00648134, 0.00659562,\n",
       "       0.00636583, 0.00668116, 0.00658988, 0.00613976, 0.00628648,\n",
       "       0.00630829, 0.00613595, 0.00600327, 0.00607892, 0.0060759 ,\n",
       "       0.0055354 , 0.00591973, 0.00577465, 0.00611906, 0.00555823,\n",
       "       0.00585435, 0.00568165, 0.00571637, 0.00570874, 0.00558875])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVD = matrices.calc_svd(PPMI,100)\n",
    "SVD = SVD - SVD.mean(axis=0) # center\n",
    "SVD = SVD / np.linalg.norm(SVD, axis=1)[:,np.newaxis] # normalize\n",
    "svd_vars = SVD.var(axis=0)/SVD.var(axis=0).sum()\n",
    "svd_vars # variance explained by each dimension (if arg two of calc_svd is None), need to normalize by all eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine these SVD representations as vectors in an embedding space: these are essentially document (paragraph in this case) embeddings. Now we can examine how different documents are distributed across the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.041524799018364346)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcualte average veector norm and norm of averagae vector\n",
    "np.linalg.norm(SVD, axis=1).mean(), np.linalg.norm(SVD.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embeddings\n",
    "Because we have document embeddings, we can measure the field that situates each document. In this case, we'll identify the document closest to the mean vector (center of the field), and then the document furthest from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation of word freq with distance: -0.8267501433369303\n",
      "norm of mean: 0.041524799018364346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-9.36939204, -7.84807113, -7.31467506, -6.98291822, -6.88569154]),\n",
       " array([10.61537976,  9.94522212,  9.22464922,  8.87196866,  8.86831118]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dists = np.linalg.norm(SVD - SVD.mean(axis=0), axis=1)\n",
    "#dists = SVD.dot(SVD.mean(axis=0))\n",
    "dists = SVD.dot(SVD.T).sum(axis=1)\n",
    "par_lens = np.array([len(toks) for toks in tokenized_pars])\n",
    "print('correlation of word freq with distance:', np.corrcoef(par_lens, dists)[0,1])\n",
    "print('norm of mean:', np.linalg.norm(SVD.mean(axis=0)))\n",
    "sortind = dists.argsort() # first is greatest dist\n",
    "dists[sortind][:5], dists[sortind[::-1]][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 332 d = 1.0172990899760197\n",
      "This strategy builds on the progress of the last 6 years, in which our active leadership has helped the world recover from a global economic crisis and respond to an array of emerging challenges. Our progress includes strengthening an unrivaled alliance system, underpinned by our enduring partnership with Europe, while investing in nascent multilateral forums like the G-20 and East Asia Summit. We brought most of our troops home after more than a decade of honorable service in two wars while adapting our counterterrorism strategy for an evolving terrorist threat. We led a multinational coalition to support the Afghan government to take responsibility for the security of their country, while supporting Afghanistans first peaceful, democratic transition of power. The United States led the international response to natural disasters, including the earthquake in Haiti, the earthquake and tsunami in Japan, and the typhoon in the Philippines to save lives, prevent greater damage, and support efforts to rebuild. We led international efforts to stop the proliferation of nuclear weapons, including by building an unprecedented international sanctions regime to hold Iran responsible for failing to meet its international obligations, while pursuing a diplomatic effort that has already stopped the progress of Irans nuclear program and rolled it back in key respects. We are rebalancing toward Asia and the Pacific while seeking new opportunities for partnership and investment in Africa and the Americas, where we have spurred greater agriculture and energy-related investments than ever before. And at home and abroad, we are taking concerted action to confront the dangers posed by climate change and to strengthen our energy security. \n",
      "\n",
      "doc 330 d = 1.0184873431932542\n",
      "On all these fronts, America leads from a position of strength. But, this does not mean we can or should attempt to dictate the trajectory of all unfolding events around the world. As powerful as we are and will remain, our resources and influence are not infinite. And in a complex world, many of the security problems we face do not lend themselves to quick and easy fixes. The United States will always defend our interests and uphold our commitments to allies and partners. But, we have to make hard choices among many competing priorities, and we must always resist the over-reach that comes when we make decisions based upon fear. Moreover, we must recognize that a smart national security strategy does not rely solely on military power. Indeed, in the long-term, our efforts to work with other countries to counter the ideology and root causes of violent extremism will be more important than our capacity to remove terrorists from the battlefield.The challenges we face require strategic patience and persistence. They require us to take our responsibilities seriously and make the smart investments in the foundations of our national power. Therefore, I will continue to pursue a comprehensive agenda that draws on all elements of our national strength, that is attuned to the strategic risks and opportunities we face, and that is guided by the principles and priorities set out in this strategy. Moreover, I will continue to insist on budgets that safeguard our strength and work with the Congress to end sequestration, which undercuts our national security.This is an ambitious agenda, and not everything will be completed during my Presidency. But I believe this is an achievable agenda, especially if we proceed with confidence and if we restore the bipartisan center that has been a pillar of strength for American foreign policy in decades past. As Americans, we will always have our differences, but what unites us is the national consensus that American global leadership remains indispensable. We embrace our exceptional role and responsibilities at a time when our unique contributions and capabilities are needed most, and when the choices we make today can mean greater security and prosperity for our Nation for decades to come.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get closest to center of field\n",
    "for i in range(2,0,-1):\n",
    "    ind = sortind[i]\n",
    "    d = np.linalg.norm(SVD[ind] - SVD.mean(axis=0))\n",
    "    print('doc', ind, 'd =', d)\n",
    "    print(par_texts[ind], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 28 d = 0.976515231358157\n",
      "Protect the American People, the Homeland, and the American Way of Life\n",
      "\n",
      "doc 191 d = 0.9780701717108031\n",
      "IMPROVE ATTRIBUTION, ACCOUNTABILITY, AND RESPONSE:  We will invest in capabilities to support and improve our ability to attribute cyberattacks, to allow for rapid response.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get closest to center of field\n",
    "for i in range(2):\n",
    "    ind = sortind[::-1][i]\n",
    "    d = np.linalg.norm(SVD[ind] - SVD.mean(axis=0))\n",
    "    print('doc', ind, 'd =', d)\n",
    "    print(par_texts[ind], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Co-Occurrence Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((524, 524), (524, 524))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_cooc = (corpus[:k].T*corpus[:k]).toarray()\n",
    "obama_cooc = (corpus[k:].T*corpus[k:]).toarray()\n",
    "trump_cooc.shape, obama_cooc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trump_pars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-684634ce512e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compute vocab from words that appear at least once in each subcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrump_tok_cts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrump_pars\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mobama_tok_cts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobama_pars\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrump_tok_cts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobama_tok_cts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trump_pars' is not defined"
     ]
    }
   ],
   "source": [
    "# compute vocab from words that appear at least once in each subcorpora\n",
    "min_count = 1\n",
    "trump_tok_cts = Counter([tok for doc in trump_pars for tok in doc])\n",
    "obama_tok_cts = Counter([tok for doc in obama_pars for tok in doc])\n",
    "words = set(trump_tok_cts.keys()) & set(obama_tok_cts.keys())\n",
    "word_cts = {w:(obama_tok_cts[w]+trump_tok_cts[w]) for w in words}\n",
    "vocab = list(sorted(word_cts.items(), key=lambda x: x[0], reverse=True))\n",
    "n = len(vocab)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Co-Occurrence Matrix\n",
    "One common technique used in corpus linguistics and other social sciences is the co-occurrence matrix. This model of the text is implicitly used in word2vec algorithms. It can be defined in a number of ways, but here we will define it as the number of times two words appear in the same paragraph together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cooccur(docs):\n",
    "    cooccur = dict()\n",
    "    for doc in docs:\n",
    "        for i,w1 in enumerate(doc):\n",
    "            for j,w2 in enumerate(doc[i:]):\n",
    "                k = (w1,w2)\n",
    "                if k not in cooccur:\n",
    "                    cooccur[k] = 0\n",
    "                cooccur[k] += 1\n",
    "    return cooccur\n",
    "trump_cooc_dict = calc_cooccur(trump_pars)\n",
    "obama_cooc_dict = calc_cooccur(obama_pars)\n",
    "print(list(trump_cooc_dict.keys())[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooc_matrix(cooc_dict, vocab):\n",
    "    voc_index = {tok:i for i,tok in enumerate(vocab)}\n",
    "    cooc_mat = np.zeros((n,n))\n",
    "    for toks,ct in cooc_dict.items():\n",
    "        w1,w2 = toks\n",
    "        cooc_mat[voc_index[w1],voc_index[w2]] = ct\n",
    "    return cooc_mat\n",
    "trump_cooc = cooc_matrix(trump_cooc_dict, vocab)\n",
    "obama_cooc = cooc_matrix(trump_cooc_dict, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
