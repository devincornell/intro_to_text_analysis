{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to run the command \"python dump_data.py gutenberg test -n 10\". Now we will grab the filenames of the documents we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 10 texts.\n"
     ]
    }
   ],
   "source": [
    "filenames = glob.glob('test/*.txt')\n",
    "texts = list()\n",
    "for fname in filenames:\n",
    "    with open(fname,'r') as f:\n",
    "        texts.append(f.read())\n",
    "print('read', len(texts), 'texts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word and Sentence Tokenizers\n",
    "We now have the filenames and texts. Next, we extract a single text file to demonstrate word and sentence tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; an\n"
     ]
    }
   ],
   "source": [
    "single_text = texts[0]\n",
    "print(single_text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(single_text)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.',\n",
       " \"She was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(single_text)\n",
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize All Texts\n",
    "Now we tokenized all texts in a loop, counting number of words and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen']\n",
      "found 191673 words in test/austen-emma.txt\n",
      "['[', 'Persuasion', 'by', 'Jane', 'Austen']\n",
      "found 97888 words in test/austen-persuasion.txt\n",
      "['[', 'Sense', 'and', 'Sensibility', 'by']\n",
      "found 141367 words in test/austen-sense.txt\n",
      "['[', 'The', 'King', 'James', 'Bible']\n",
      "found 946812 words in test/bible-kjv.txt\n",
      "['[', 'Poems', 'by', 'William', 'Blake']\n",
      "found 8239 words in test/blake-poems.txt\n",
      "['[', 'Stories', 'to', 'Tell', 'to']\n",
      "found 55621 words in test/bryant-stories.txt\n",
      "['[', 'The', 'Adventures', 'of', 'Buster']\n",
      "found 18542 words in test/burgess-busterbrown.txt\n",
      "['[', 'Alice', \"'s\", 'Adventures', 'in']\n",
      "found 33310 words in test/carroll-alice.txt\n",
      "['[', 'The', 'Ball', 'and', 'The']\n",
      "found 97203 words in test/chesterton-ball.txt\n",
      "['[', 'The', 'Wisdom', 'of', 'Father']\n",
      "found 85412 words in test/chesterton-brown.txt\n"
     ]
    }
   ],
   "source": [
    "for fname,text in zip(filenames, texts):\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    print('found', len(words), 'words in', fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 7493 sentences in test/austen-emma.txt\n",
      "found 3654 sentences in test/austen-persuasion.txt\n",
      "found 4833 sentences in test/austen-sense.txt\n",
      "found 29812 sentences in test/bible-kjv.txt\n",
      "found 355 sentences in test/blake-poems.txt\n",
      "found 2715 sentences in test/bryant-stories.txt\n",
      "found 1001 sentences in test/burgess-busterbrown.txt\n",
      "found 1625 sentences in test/carroll-alice.txt\n",
      "found 4624 sentences in test/chesterton-ball.txt\n",
      "found 3712 sentences in test/chesterton-brown.txt\n"
     ]
    }
   ],
   "source": [
    "for fname,text in zip(filenames, texts):\n",
    "    sents = nltk.tokenize.sent_tokenize(text)\n",
    "    print('found', len(sents), 'sentences in', fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Length\n",
    "Now try identifying the average sentence length. We will need to first tokenize by sentence (sent_tokenize), then by words (word_tokenize on each sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/austen-emma.txt\n",
      " number: 7493; max words: 275; min words: 1; av words: 25.58054183904978\n",
      "\n",
      "test/austen-persuasion.txt\n",
      " number: 3654; max words: 215; min words: 1; av words: 26.78927203065134\n",
      "\n",
      "test/austen-sense.txt\n",
      " number: 4833; max words: 346; min words: 2; av words: 29.250362093937515\n",
      "\n",
      "test/bible-kjv.txt\n",
      " number: 29812; max words: 564; min words: 2; av words: 31.759425734603514\n",
      "\n",
      "test/blake-poems.txt\n",
      " number: 355; max words: 105; min words: 2; av words: 23.208450704225353\n",
      "\n",
      "test/bryant-stories.txt\n",
      " number: 2715; max words: 135; min words: 2; av words: 20.486556169429097\n",
      "\n",
      "test/burgess-busterbrown.txt\n",
      " number: 1001; max words: 93; min words: 2; av words: 18.523476523476525\n",
      "\n",
      "test/carroll-alice.txt\n",
      " number: 1625; max words: 202; min words: 2; av words: 20.498461538461537\n",
      "\n",
      "test/chesterton-ball.txt\n",
      " number: 4624; max words: 202; min words: 2; av words: 21.021410034602077\n",
      "\n",
      "test/chesterton-brown.txt\n",
      " number: 3712; max words: 115; min words: 2; av words: 23.009698275862068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fname,text in zip(filenames, texts):\n",
    "    sents = nltk.tokenize.sent_tokenize(text)\n",
    "    sent_words = [nltk.tokenize.word_tokenize(s) for s in sents]\n",
    "    \n",
    "    sent_lengths = [len(s) for s in sent_words]\n",
    "    max_len = max(sent_lengths)\n",
    "    min_len = min(sent_lengths)\n",
    "    av_len = sum(sent_lengths)/len(sent_lengths)\n",
    "    \n",
    "    print('{}\\n number: {}; max words: {}; '\n",
    "          'min words: {}; av words: {}'\n",
    "          '\\n'.format(fname, len(sents), max_len, min_len, av_len)\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Word Frequencies\n",
    "In this step, we will count how often each word appears in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/austen-emma.txt\n",
      "[('Austen', 1), ('1816', 1), ('twenty-one', 1)]\n",
      "[('to', 5124), ('.', 6355), (',', 12016)]\n",
      "\n",
      "test/austen-persuasion.txt\n",
      "[('[', 1), ('Persuasion', 1), ('Jane', 1)]\n",
      "[('the', 3118), ('.', 3119), (',', 7024)]\n",
      "\n",
      "test/austen-sense.txt\n",
      "[('Sense', 1), ('Sensibility', 1), ('Jane', 1)]\n",
      "[('.', 4023), ('to', 4050), (',', 9901)]\n",
      "\n",
      "test/bible-kjv.txt\n",
      "[('[', 1), (']', 1), ('Old', 1)]\n",
      "[('and', 38847), ('the', 62103), (',', 70573)]\n",
      "\n",
      "test/blake-poems.txt\n",
      "[('[', 1), ('Poems', 1), ('1789', 1)]\n",
      "[('.', 221), ('the', 351), (',', 685)]\n",
      "\n",
      "test/bryant-stories.txt\n",
      "[('Stories', 1), ('Sara', 1), ('Cone', 1)]\n",
      "[('.', 2049), ('the', 3086), (',', 3855)]\n",
      "\n",
      "test/burgess-busterbrown.txt\n",
      "[('Adventures', 1), ('Thornton', 1), ('W.', 1)]\n",
      "[('the', 639), ('.', 843), (',', 886)]\n",
      "\n",
      "test/carroll-alice.txt\n",
      "[('Lewis', 1), ('Carroll', 1), ('1865', 1)]\n",
      "[(\"'\", 1127), ('the', 1516), (',', 2418)]\n",
      "\n",
      "test/chesterton-ball.txt\n",
      "[('1909', 1), ('SOMEWHAT', 1), ('AIR', 1)]\n",
      "[('.', 3997), ('the', 4521), (',', 5223)]\n",
      "\n",
      "test/chesterton-brown.txt\n",
      "[('[', 1), ('Wisdom', 1), ('G.', 1)]\n",
      "[('.', 3253), ('the', 4316), (',', 4647)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fname,text in zip(filenames, texts):\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    sort_word_counts = list(sorted(word_counts.items(), key=lambda x: x[1]))\n",
    "    \n",
    "    print(fname)\n",
    "    print(sort_word_counts[:3])\n",
    "    print(sort_word_counts[-3:])\n",
    "    print()\n",
    "    #print('found', len(words), 'words in', fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) Tagging\n",
    "\n",
    "POS tagging assigns to each token a part of speech indicating it's grammatical role in the sentence structure.\n",
    "\n",
    "This page includes more information about the tags themselves: [POS tag reference](https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /cs/student/dcornell/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/austen-emma.txt\n",
      "[('[', 'NNS'), ('Emma', 'NNP'), ('by', 'IN')]\n",
      "\n",
      "test/austen-persuasion.txt\n",
      "[('[', 'JJ'), ('Persuasion', 'NNP'), ('by', 'IN')]\n",
      "\n",
      "test/austen-sense.txt\n",
      "[('[', 'JJ'), ('Sense', 'NNP'), ('and', 'CC')]\n",
      "\n",
      "test/bible-kjv.txt\n",
      "[('[', 'VB'), ('The', 'DT'), ('King', 'NNP')]\n",
      "\n",
      "test/blake-poems.txt\n",
      "[('[', 'JJ'), ('Poems', 'NNP'), ('by', 'IN')]\n",
      "\n",
      "test/bryant-stories.txt\n",
      "[('[', 'NN'), ('Stories', 'NNS'), ('to', 'TO')]\n",
      "\n",
      "test/burgess-busterbrown.txt\n",
      "[('[', 'IN'), ('The', 'DT'), ('Adventures', 'NNP')]\n",
      "\n",
      "test/carroll-alice.txt\n",
      "[('[', 'JJ'), ('Alice', 'NNP'), (\"'s\", 'POS')]\n",
      "\n",
      "test/chesterton-ball.txt\n",
      "[('[', 'IN'), ('The', 'DT'), ('Ball', 'NNP')]\n",
      "\n",
      "test/chesterton-brown.txt\n",
      "[('[', 'IN'), ('The', 'DT'), ('Wisdom', 'NNP')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fname,text in zip(filenames, texts):\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    \n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    print(fname)\n",
    "    print(pos_tags[:5])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
